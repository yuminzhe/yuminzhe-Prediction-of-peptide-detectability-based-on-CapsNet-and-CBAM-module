{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al3Xe5wkve0s"
      },
      "source": [
        "# 1. Extract data\n",
        "\n",
        ">1|0<br>\n",
        "AAYLLAKINLKALAALAKKIL<br>\n",
        ">2|0<br>\n",
        "AEKVDPVKLNLTLSAAAEALTGLGDK<br>\n",
        ">3|0<br>\n",
        "AGYLLGKINLKALAALAKKIL<br>\n",
        "...<br>\n",
        ">372|1<br>\n",
        "TKKDLTQWFFKITDYADELLDKLD<br>\n",
        ">373|1<br>\n",
        "SGWNAYIDTMTAAAP<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "xZB3HGozvfNE",
        "outputId": "2235c2dd-4a07-4f78-a696-90e43bde01ac"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "filename = 'CPP.txt'\n",
        "x = []\n",
        "y = []\n",
        "with open(filename) as f:\n",
        "    for index,line in enumerate(f.readlines()):\n",
        "        if(index%2==0):\n",
        "            label = int(line.split('|')[1])\n",
        "            y.append(label)\n",
        "        \n",
        "        else:\n",
        "            x.append(line.strip('\\n'))\n",
        "\n",
        "c={\"Seqs\" : x,\n",
        "   \"Label\" : y}\n",
        "cppdata=pd.DataFrame(c)\n",
        "cppdata.to_csv(\"CPP.csv\",index=False)\n",
        "cppdata.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Seqs</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAAAARRRIRKQAHAHSK</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ADVFDRGGPYLQRGVADLVPTATLLDTYSP</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AGCKNFFWKTFTSC</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AGYLLGKLKALAALAKKIL</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AHALCLTERQIKSNRRMKWKKEN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Seqs  Label\n",
              "0              AAAAARRRIRKQAHAHSK      1\n",
              "1  ADVFDRGGPYLQRGVADLVPTATLLDTYSP      1\n",
              "2                  AGCKNFFWKTFTSC      1\n",
              "3             AGYLLGKLKALAALAKKIL      1\n",
              "4         AHALCLTERQIKSNRRMKWKKEN      1"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfPCPTO-ztId"
      },
      "source": [
        "# 2. Divide training set and test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCCkkzWjwRX-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "# create train and test set \n",
        "kf = KFold(n_splits=10,shuffle=True)\n",
        "a= 0\n",
        "for train, test in kf.split(cppdata):\n",
        "  a=a+1 \n",
        "  temptrain = cppdata.iloc[train]\n",
        "  temptest = cppdata.iloc[test]\n",
        "  temptrain.to_csv(\"train%d.csv\" %a, index=False)\n",
        "  temptest.to_csv(\"test%d.csv\" %a, index=False)\n",
        "# train, test = train_test_split(cppdata, test_size=0.1)\n",
        "# train.to_csv(\"train.csv\", index=False)\n",
        "# test.to_csv(\"test.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWBAcEQv0Fez"
      },
      "source": [
        "# 3. Preprocess\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IevxcQX9w6oF"
      },
      "source": [
        "import spacy\n",
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "from torchtext.vocab import Vectors\n",
        "from torch.nn import init\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXZxsFQa0LMP"
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenizer(text): # create a tokenizer function\n",
        "    \"\"\"\n",
        "    定义分词操作\n",
        "    \"\"\"\n",
        "    return list(text)\n",
        "\n",
        "\"\"\"\n",
        "field在默认的情况下都期望一个输入是一组单词的序列，并且将单词映射成整数。\n",
        "这个映射被称为vocab。如果一个field已经被数字化了并且不需要被序列化，\n",
        "可以将参数设置为use_vocab=False以及sequential=False。\n",
        "\"\"\"\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer,fix_length=45)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQQEhZkN0c99"
      },
      "source": [
        "train,val,test = data.TabularDataset.splits(\n",
        "        path='.', train='train.csv',validation='test.csv',test='test.csv', format='csv',skip_header=True,\n",
        "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cbs89Z0ean",
        "outputId": "4e71d6a2-dbcd-42e7-b3d6-f6bf15f2c184"
      },
      "source": [
        "print(train[5])\n",
        "print(train[5].__dict__.keys())\n",
        "print(train[5].Seqs,train[5].Label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torchtext.legacy.data.example.Example object at 0x7f039089fa50>\n",
            "dict_keys(['Seqs', 'Label'])\n",
            "['L', 'A', 'V', 'N', 'M', 'V', 'P', 'F', 'P', 'R'] 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HU3H4qB-08mN"
      },
      "source": [
        "TEXT.build_vocab(train,val,test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKvWW1JE1GL1",
        "outputId": "994e8d1a-2c8e-4483-eddc-46e394437997"
      },
      "source": [
        "print(TEXT.vocab.itos[1])\n",
        "print(TEXT.vocab.stoi['N'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pad>\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnkUfE981XVM"
      },
      "source": [
        "train_iter = data.BucketIterator(train, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
        "                                 shuffle=True,device=DEVICE)\n",
        "\n",
        "val_iter = data.BucketIterator(val, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
        "                                 shuffle=True,device=DEVICE)\n",
        "\n",
        "test_iter = data.BucketIterator(val, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
        "                                 shuffle=True,device=DEVICE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEozgl3s2Mcq"
      },
      "source": [
        "# 4. Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObFiT7jXqsFU"
      },
      "source": [
        "epsilon = 0.00000001\n",
        "def squash(x):\n",
        "    # not concern batch_size, maybe rewrite\n",
        "    s_squared_norm = torch.sum(x*x,1,keepdim=True) + epsilon\n",
        "    scale = torch.sqrt(s_squared_norm)/(1. + s_squared_norm)\n",
        "    # out = (batch_size,1,10)*(batch_size,16,10) = (batch_size,16,10)\n",
        "    out = scale * x\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_kJkmS-qxIl"
      },
      "source": [
        "class Capsule(nn.Module):\n",
        "\n",
        "    def __init__(self, in_units,in_channels, num_capsule, dim_capsule, routings=3, **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.in_units = in_units\n",
        "        self.in_channels = in_channels\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        # (in_units,10,128,16)\n",
        "        self.W = nn.Parameter((torch.randn(self.in_units,self.num_capsule,self.in_channels, self.dim_capsule)))\n",
        "\n",
        "    def forward(self, u_vecs):\n",
        "        u_vecs = u_vecs.permute(0,2,1)\n",
        "        u_vecs = u_vecs.unsqueeze(2)\n",
        "        u_vecs = u_vecs.unsqueeze(2)\n",
        "\n",
        "        \n",
        "        # (batch_size,in_units,1,1,in_channels)*(in_units,10,in_channels,16) = (batch_size,in_units,10,1,16)\n",
        "        u_hat_vecs = torch.matmul(u_vecs,self.W)\n",
        "        # (batch_size,in_units,10,16)\n",
        "        u_hat_vecs = u_hat_vecs.permute(0,1,2,4,3).squeeze(4)\n",
        "        \n",
        "        # (batch_size,10,in_units,16)\n",
        "        u_hat_vecs2 = u_hat_vecs.permute(0,2,1,3)\n",
        "    \n",
        "        # (batch_size,10,1,in_units)\n",
        "        b = torch.zeros(u_hat_vecs.size(0),self.num_capsule,1,self.in_units,device=DEVICE)\n",
        "        for i in range(self.routings):\n",
        "            # (batch_size,10,1,in_units)\n",
        "            c = F.softmax(b,-1)\n",
        "            # s = (batch_size,10,1,in_units)*(batch_size,10,in_units,16) = (batch_size,10,1,16)\n",
        "            s = torch.matmul(c,u_hat_vecs2)\n",
        "            # (batch_size,16,10)\n",
        "            s = s.permute(0,3,1,2).squeeze(3)\n",
        "            # (batch_size,16,10)\n",
        "            v = squash(s)\n",
        "            # here\n",
        "            # (batch_size,10,16,1)\n",
        "            v = v.permute(0,2,1).unsqueeze(3)\n",
        "            # (batch_size,10,in_units,16)*(batch_size,10,16,1) = (batch_size,10,in_units,1)\n",
        "            sim = torch.matmul(u_hat_vecs2,v)\n",
        "            # (batch_size,10,1,in_units)\n",
        "            sim = sim.permute(0,1,3,2)\n",
        "            b = b+sim\n",
        "        # (batch_size,16,10)\n",
        "        return v.permute(0,2,1,3).squeeze(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnDBKedYQE_f"
      },
      "source": [
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfjBegd-1-j0"
      },
      "source": [
        "len_vocab = len(TEXT.vocab)\n",
        "embed_size=20\n",
        "n_class=2\n",
        "n_hidden =32\n",
        "\n",
        "energy = [\n",
        "[-1.65,  -2.83, 1.16,\t1.80,\t-3.73,\t-0.41,\t1.90,\t-3.69,\t0.49,\t-3.01,\t-2.08,\t0.66,\t1.54,\t1.20,\t0.98, -0.08,  0.46, -2.31,\t0.32,\t-4.62],\n",
        "[-2.83,\t-39.58,\t-0.82,\t-0.53,\t-3.07,\t-2.96,\t-4.98,\t0.34,\t-1.38,\t-2.15,\t1.43,\t-4.18,\t-2.13,\t-2.91,\t-0.41,\t-2.33,\t-1.84,\t-0.16,\t4.26,\t-4.46],\n",
        "[1.16,\t-0.82,\t0.84,\t1.97,\t-0.92,\t0.88,\t-1.07,\t0.68,\t-1.93,\t0.23,\t0.61,\t0.32,\t3.31,\t2.67,\t-2.02,\t0.91,\t-0.65,\t0.94,\t-0.71,\t0.90],\n",
        "[1.80,\t-0.53,\t1.97,\t1.45,\t0.94,\t1.31,\t0.61,\t1.30,\t-2.51,\t1.14,\t2.53,\t0.20,\t1.44,\t0.10,\t-3.13,\t0.81,\t1.54,\t0.12,\t-1.07,\t1.29],\n",
        "[-3.73,\t-3.07,\t-0.92,\t0.94,\t-11.25,\t0.35,\t-3.57,\t-5.88,\t-0.82,\t-8.59,\t-5.34,\t0.73,\t0.32,\t0.77,\t-0.40,\t-2.22,\t0.11,\t-7.05,\t-7.09,\t-8.80],\n",
        "[-0.41,\t-2.96,\t0.88,\t1.31,\t0.35,\t-0.20,\t1.09,\t-0.65,\t-0.16,\t-0.55,\t-0.52,\t-0.32,\t2.25,\t1.11,\t0.84,\t0.71,\t0.59,\t-0.38,\t1.69,\t-1.90],\n",
        "[1.90,\t-4.98,\t-1.07,\t0.61,\t-3.57,\t1.09,\t1.97,\t-0.71,\t2.89,\t-0.86,\t-0.75,\t1.84,\t0.35,\t2.64,\t2.05,\t0.82,\t-0.01,\t0.27,\t-7.58,\t-3.20],\n",
        "[-3.69,\t0.34,\t0.68,\t1.30,\t-5.88,\t-0.65,\t-0.71,\t-6.74,\t-0.01,\t-9.01,\t-3.62,\t-0.07,\t0.12,\t-0.18,\t0.19,\t-0.15,\t0.63,\t-6.54,\t-3.78,\t-5.26],\n",
        "[0.49,\t-1.38,\t-1.93,\t-2.51,\t-0.82,\t-0.16,\t2.89,\t-0.01,\t1.24,\t0.49,\t1.61,\t1.12,\t0.51,\t0.43,\t2.34,\t0.19,\t-1.11,\t0.19,\t0.02,\t-1.19],\n",
        "[-3.01,\t-2.15,\t0.23,\t1.14,\t-8.59,\t-0.55,\t-0.86,\t-9.01,\t0.49,\t-6.37,\t-2.88,\t0.97,\t1.81,\t-0.58,\t-0.60,\t-0.41,\t0.72,\t-5.43,\t-8.31,\t-4.90],\n",
        "[-2.08,\t1.43,\t0.61,\t2.53,\t-5.34,\t-0.52,\t-0.75,\t-3.62,\t1.61,\t-2.88,\t-6.49,\t0.21,\t0.75,\t1.90,\t2.09,\t1.39,\t0.63,\t-2.59,\t-6.88,\t-9.73],\n",
        "[0.66,\t-4.18,\t0.32,\t0.20,\t0.73,\t-0.32,\t1.84,\t-0.07,\t1.12,\t0.97,\t0.21,\t0.61,\t1.15,\t1.28,\t1.08,\t0.29,\t0.46,\t0.93,\t-0.74,\t0.93],\n",
        "[1.54,\t-2.13,\t3.31,\t1.44,\t0.32,\t2.25,\t0.35,\t0.12,\t0.51,\t1.81,\t0.75,\t1.15,\t-0.42,\t2.97,\t1.06,\t1.12,\t1.65,\t0.38,\t-2.06,\t-2.09],\n",
        "[1.20,\t-2.91,\t2.67,\t0.10,\t0.77,\t1.11,\t2.64,\t-0.18,\t0.43,\t-0.58,\t1.90,\t1.28,\t2.97,\t-1.54,\t0.91,\t0.85,\t-0.07,\t-1.91,\t-0.76,\t0.01],\n",
        "[0.98,\t-0.41,\t-2.02,\t-3.13,\t-0.40,\t0.84,\t2.05,\t0.19,\t2.34,\t-0.60,\t2.09,\t1.08,\t1.06,\t0.91,\t0.21,\t0.95,\t0.98,\t0.08,\t-5.89,\t0.36],\n",
        "[-0.08,\t-2.33,\t0.91,\t0.81,\t-2.22,\t0.71,\t0.82,\t-0.15,\t0.19,\t-0.41,\t1.39,\t0.29,\t1.12,\t0.85,\t0.95,\t-0.48,\t-0.06,\t0.13,\t-3.03,\t-0.82],\n",
        "[0.46,\t-1.84,\t-0.65,\t1.54,\t0.11,\t0.59,\t-0.01,\t0.63,\t-1.11,\t0.72,\t0.63,\t0.46,\t1.65,\t-0.07,\t0.98,\t-0.06,\t-0.96,\t1.14,\t-0.65,\t-0.37],\n",
        "[-2.31,\t-0.16,\t0.94,\t0.12,\t-7.05,\t-0.38,\t0.27,\t-6.54,\t0.19,\t-5.43,\t-2.59,\t0.93,\t0.38,\t-1.91,\t0.08,\t0.13,\t1.14,\t-4.82,\t-2.13,\t-3.59],\n",
        "[0.32,\t4.26,\t-0.71,\t-1.07,\t-7.09,\t1.69,\t-7.58,\t-3.78,\t0.02,\t-8.31,\t-6.88,\t-0.74,\t-2.06,\t-0.76,\t-5.89,\t-3.03,\t-0.65,\t-2.13,\t-1.73,\t-12.39],\n",
        "[-4.62,\t-4.46,\t0.90,\t1.29,\t-8.80,\t-1.90,\t-3.20,\t-5.26,\t-1.19,\t-4.90,\t-9.73,\t0.93,\t-2.09,\t0.01,\t0.36,\t-0.82,\t-0.37,\t-3.59,\t-12.39,\t-2.68],\n",
        "]\n",
        "\n",
        "physicochemical = [\n",
        "    [-0.4, -0.5, 15, 8.1, 0.046, 0.67, 1.28, 0.3, 0, 0.687, 115, 0.28, 154.330012, 27.5, 1.181, 0.0072,0,0,0,0],\n",
        "    [0.17, -1, 47, 5.5, 0.128, 0.38, 1.77, 0.9, 2.75, 0.263, 135, 0.28, 219.789, 44.6, 1.461, -0.037,0,0,0,0],\n",
        "    [-1.31, 3.0, 59, 13.0, 0.105, -1.2, 1.6, -0.6, 1.38, 0.632, 150, 0.21, 194.910002, 40.0, 1.587, 0.0238,0,0,0,0],\n",
        "    [-1.22, 3.0, 73, 12.3, 0.151, -0.76, 1.56, -0.7, 0.92, 0.669, 190, 0.33, 223.160, 62, 1.862, 0.0068,0,0,0,0],\n",
        "    [1.92, -2.5, 91, 5.2, 0.29, 2.3, 2.94, 0.5, 0, 0.577, 210, 2.18, 204.7, 115.5, 2.228, 0.0376,0,0,0,0],\n",
        "    [-0.67, 0, 1, 9, 0, 0, 0, 0.3, 0.74, 0.67, 75, 0.18, 127.9, 0, 0.881, 0.179,0,0,0,0],\n",
        "    [-0.64, -0.5, 82, 10.4, 0.23, 0.64, 2.99, -0.1, 0.58, 0.594, 195, 0.21, 242.539, 79, 2.025, -0.011,0,0,0,0],\n",
        "    [1.25, -1.5, 57, 5.2, 0.186, 1.9, 4.19, 0.7, 0, 0.564, 175, 0.82, 233.210, 93.5, 1.81, 0.0216,0,0,0,0],\n",
        "    [-0.67, 3, 73, 11.3, 0.219, -0.57, 1.89, -1.8, 0.33, 0.407, 200, 0.09, 300.459, 100, 2.258, 0.0177,0,0,0,0],\n",
        "    [1.22, -1.8, 57, 4.9, 0.186, 1.9, 2.59, 0.5, 0, 0.541, 170, 1, 232.3, 93.5, 1.931, 0.0517,0,0,0,0],\n",
        "    [1.02, -1.3, 75, 5.7, 0.0221, 2.4, 2.35, 0.4, 0, 0.328, 185, 0.74, 202.699, 94.1, 2.034, 0.0027,0,0,0,0],\n",
        "    [-0.92, 0.2, 58, 11.6, 0.134, -0.61, 1.6, -0.5, 1.33, 0.489, 160, 0.25, 207.899, 58.7, 1.655, 0.0054,0,0,0,0],\n",
        "    [-0.49, 0, 42, 8.0, 0.131, 102, 2.67, -0.3, 0.39, 0.600, 145, 0.39, 179.929, 41.9, 1.468, 0.239,0,0,0,0],\n",
        "    [-0.91, 0.2, 72, 10.5, 0.180, -0.22, 1.56, -0.7, 0.9, 0.527, 183, 0.35, 235.509, 80.7, 1.932, 0.0692,0,0,0,0],\n",
        "    [-0.59, 3, 101, 10.5, 0.291, -2.10, 2.34, -1.4, 0.64, 0.591, 225, 0.1, 341.0, 105, 2.56, 0.0436,0,0,0,0],\n",
        "    [-0.55, 0.3, 31, 9.2, 0.062, 0.01, 1.31, -0.1, 1.41, 0.693, 116, 0.12, 174.059, 29.3, 1.298, 0.0043,0,0,0,0],\n",
        "    [-0.28, -0.4, 45, 8.6, 0.108, 0.52, 3.03, -0.2, 0.71, 0.713, 142, 0.21, 205.5, 51.3, 1.525, 0.034,0,0,0,0],\n",
        "    [0.91, -1.5, 43, 5.9, 0.14, 1.5, 3.67, 0.6, 0, 0.529, 157, 0.6, 207, 71.5, 1.645, 0.057,0,0,0,0],\n",
        "    [0.5, -3.4, 130, 5.4, 0.409, 2.6, 3.21, 0.3, 0.12, 0.632, 258, 5.7, 237, 145.5, 2.663, 0.058,0,0,0,0],\n",
        "    [1.67, -2.3, 107, 6.2, 0.298, 1.6, 2.94, -0.4, 0.21, 0.493, 234, 1.26, 229.14, 117.3, 2.368, 0.0236,0,0,0,0]\n",
        "]\n",
        "\n",
        "RE = {'A':0,'C':1,'D':2,'E':3,'F':4,'G':5,'H':6,'I':7,'K':8,'L':9,'M':10,'N':11,'P':12,'Q':13,'R':14,'S':15,'T':16,'V':17,'W':18,'Y':19}\n",
        "def RECMEncoding(inpStr):\n",
        "  RECMT=[]\n",
        "  for x in inpStr:\n",
        "    if x in RE:\n",
        "      oneTi = energy[RE.get(x)]\n",
        "      RECMT.append(oneTi)\n",
        "  return RECMT\n",
        "    \n",
        "def RECMcompositionEncoding(inpStr):\n",
        "  RECMcomposition=[]\n",
        "  countNum = {'A':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':0,'K':0,'L':0,'M':0,'N':0,'P':0,'Q':0,'R':0,'S':0,'T':0,'V':0,'W':0,'Y':0}\n",
        "  for i in inpStr:\n",
        "    if i in countNum:\n",
        "      value = countNum.get(i)+1\n",
        "      countNum[i] = value\n",
        "  for i in countNum:\n",
        "    oneTi = np.array(energy[RE.get(i)])*int(countNum.get(i))\n",
        "    # a = GetPseRECM(RECMEncoding(i))\n",
        "    # i = np.concatenate((oneTi,a),0)\n",
        "    RECMcomposition.append(oneTi)\n",
        "  RECMcomposition = np.array(RECMcomposition)\n",
        "  return RECMcomposition\n",
        "      \n",
        "def GetPseRECM(RECMT):\n",
        "  feature =[]\n",
        "  legth = 0\n",
        "  r = 3\n",
        "  legth = 20+20*(r-1)\n",
        "  # 取平均特征\n",
        "  for j in range(20):\n",
        "    averageColumn = 0\n",
        "    for i in range(len(RECMT)):\n",
        "      averageColumn = averageColumn + RECMT[i][j]\n",
        "    averageColumn = averageColumn/len(RECMT)\n",
        "    feature.append(averageColumn)\n",
        "  for k in range(1,r):\n",
        "    for j in range(20):\n",
        "      dist = 0\n",
        "      for i in range(len(RECMT)-k):\n",
        "        dist = dist +pow((RECMT[i][j]-RECMT[i+k][j]),2)\n",
        "      dist = dist/(len(RECMT)-k)\n",
        "      feature.append(dist)\n",
        "  feature = np.array(feature)\n",
        "  return feature\n",
        "\n",
        "def residueRatio(inpStr):\n",
        "    feature = []\n",
        "    countNum = {'A':0,'C':0,'D':0,'E':0,'F':0,'G':0,'H':0,'I':0,'K':0,'L':0,'M':0,'N':0,'P':0,'Q':0,'R':0,'S':0,'T':0,'V':0,'W':0,'Y':0}\n",
        "    total = 0\n",
        "    for i in inpStr:\n",
        "        total = total+1\n",
        "        if i in countNum:\n",
        "            value = countNum.get(i)+1\n",
        "            countNum[i] = value\n",
        "    for i in countNum:\n",
        "        oneResidueRatio = countNum.get(i)#/total\n",
        "        feature.append(oneResidueRatio)\n",
        "    feature = np.array(feature)\n",
        "    return feature\n",
        "\n",
        "\n",
        "\n",
        "def dipeptideRatio(inpStr):\n",
        "    # print(inpStr)\n",
        "    dipeptideFeature = np.zeros((20,20))\n",
        "    total = 0\n",
        "    for i in range(len(inpStr)-1):\n",
        "        total = total+1\n",
        "        x = RE.get(inpStr[i])\n",
        "        y = RE.get(inpStr[i+1])\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "        dipeptideFeature[x][y] = dipeptideFeature[x][y] +1\n",
        "    # dipeptideFeature = dipeptideFeature/total\n",
        "    return dipeptideFeature\n",
        "\n",
        "\n",
        "def physicochemicalFeature(inpStr,fixlength):\n",
        "    pfeature=[]\n",
        "    Slength = 0\n",
        "    fix = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "    for x in inpStr:\n",
        "        if x in RE:\n",
        "            Slength = Slength+1\n",
        "            oneTi = physicochemical[RE.get(x)]\n",
        "            pfeature.append(oneTi)\n",
        "    fixlength = fixlength-Slength\n",
        "    for i in range(fixlength):\n",
        "        pfeature.append(fix)\n",
        "    pfeature = np.array(pfeature)\n",
        "    return pfeature\n",
        "\n",
        "def featureGenera(t):\n",
        "  flag =0 \n",
        "  for i in t:\n",
        "    protein = ''   \n",
        "    flag = flag+1\n",
        "    for j in i:\n",
        "      if j!=1:\n",
        "        a = TEXT.vocab.itos[j]\n",
        "        protein = protein+a\n",
        "    # print(protein)\n",
        "    featureOne = RECMcompositionEncoding(protein)\n",
        "    # print(featureOne.shape)\n",
        "    featureTwo = GetPseRECM(RECMEncoding(protein))\n",
        "    featureThree = dipeptideRatio(protein)\n",
        "    featureFour = residueRatio(protein)\n",
        "    featureFive = physicochemicalFeature(protein,45)\n",
        "    # print(featureTwo.shape)\n",
        "    featureTwo =featureTwo.reshape(3,20)\n",
        "    featureFour = featureFour.reshape(1,20)\n",
        "    featureOne = torch.from_numpy(featureOne)\n",
        "    featureTwo = torch.from_numpy(featureTwo)\n",
        "    featureThree = torch.from_numpy(featureThree)\n",
        "    featureFour = torch.from_numpy(featureFour)\n",
        "    featureFive = torch.from_numpy(featureFive)\n",
        "    featureThree.type_as(featureTwo)\n",
        "    featureFour.type_as(featureTwo)\n",
        "    featureFive.type_as(featureTwo)\n",
        "    # print(featureOne.shape)\n",
        "    # print(featureTwo.shape)\n",
        "    # print(featureThree.shape)\n",
        "    # print(featureFour.shape)\n",
        "    feature1 = torch.cat((featureOne,featureTwo),0)\n",
        "    # print(feature1.shape)\n",
        "    feature2 = torch.cat((featureThree,featureFour.type_as(featureThree)),0)\n",
        "    # # print(feature2.shape)\n",
        "    feature3 = torch.cat((feature1,feature2),0)\n",
        "    feature = torch.cat((feature3,featureFive),0)\n",
        "\n",
        "    # print(feature.shape)\n",
        "    # print(feature.shape)\n",
        "    if(flag==1):\n",
        "      # print(feature.shape)\n",
        "      feature = feature.unsqueeze(0)\n",
        "      temp = feature\n",
        "    if(flag!=1):\n",
        "      # print(feature.shape)\n",
        "      feature = feature.unsqueeze(0)\n",
        "      temp = torch.cat((temp,feature),0)\n",
        "  # print(temp.shape)\n",
        "  return temp\n",
        "\n",
        "\n",
        "class Enet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Enet, self).__init__()\n",
        "        self.embedding = nn.Embedding(len_vocab,embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size,64,batch_first=True)#,bidirectional=True)\n",
        "        self.conv = nn.Conv1d(embed_size,32,3)\n",
        "        self.pool = nn.MaxPool1d(32)\n",
        "        self.linear = nn.Linear(64,n_class)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size,seq_num = x.shape\n",
        "        y = featureGenera(x)\n",
        "        # print(x.shape)\n",
        "        vec = self.embedding(x)\n",
        "        # print(vec.shape)\n",
        "        # vec = torch.cat((vec,y.type_as(vec)),1) \n",
        "        out, (hn, cn) = self.lstm(vec)\n",
        "        # print(out.shape)\n",
        "        #out = self.conv(vec.permute(0,2,1))\n",
        "        out = F.relu(out)\n",
        "        out = self.linear(out[:,-1,:])\n",
        "        # out = self.linear(out)\n",
        "        out = F.softmax(out,-1)\n",
        "        return out\n",
        "\n",
        "class CBAMBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(CBAMBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        self.channel_excitation = nn.Sequential(nn.Linear(channel,int(channel//reduction),bias=False),\n",
        "                                                nn.ReLU(inplace=True),\n",
        "                                                nn.Linear(int(channel//reduction),channel,bias=False),\n",
        "                                                )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.spatial_excitation = nn.Sequential(nn.Conv2d(2, 1, kernel_size=7,\n",
        "                                                 stride=1, padding=3, bias=False),\n",
        "                                               )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bahs, chs, _, _ = x.size()\n",
        "\n",
        "        # Returns a new tensor with the same data as the self tensor but of a different size.\n",
        "        chn_avg = self.avg_pool(x).view(bahs, chs)\n",
        "        chn_avg = self.channel_excitation(chn_avg).view(bahs, chs, 1, 1)\n",
        "        chn_max = self.max_pool(x).view(bahs, chs)\n",
        "        chn_max = self.channel_excitation(chn_max).view(bahs, chs, 1, 1)\n",
        "        chn_add=chn_avg+chn_max\n",
        "        chn_add=self.sigmoid(chn_add)\n",
        "\n",
        "        chn_cbam = torch.mul(x, chn_add)\n",
        "\n",
        "        avg_out = torch.mean(chn_cbam, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(chn_cbam, dim=1, keepdim=True)\n",
        "        cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        spa_add = self.spatial_excitation(cat)\n",
        "        spa_add=self.sigmoid(spa_add)\n",
        "\n",
        "        spa_cbam = torch.mul(chn_cbam, spa_add)\n",
        "        return spa_cbam\n",
        "\n",
        "\n",
        "class CapsuleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 1,28x28\n",
        "        self.embedding = nn.Embedding(len_vocab,embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size,40,batch_first=True)#,bidirectional=True)\n",
        "        self.conv1=nn.Conv2d(1,256,9)\n",
        "        self.conv3=nn.Conv2d(1,256,8)\n",
        "        self.cbamBlock = CBAMBlock(256) \n",
        "        self.conv2=nn.Conv2d(256,32*8,9,2)\n",
        "        self.conv4=nn.Conv2d(256,32*8,8,2) \n",
        "        self.capsule = Capsule(2304,16,2,32)\n",
        "        # self.Decoder = Decoder()\n",
        "   \n",
        "    def forward(self,x):\n",
        "        batch_size = x.size(0)\n",
        "        # Conv1\n",
        "        # print(x)\n",
        "        y = featureGenera(x)\n",
        "        # print(y.shape)\n",
        "        out = self.embedding(x)\n",
        "        # print(out.shape)\n",
        "        #out, (hn, cn) = self.lstm(out)\n",
        "        # print(out.shape)\n",
        "        #out = out[:,-1,:].reshape(batch_size,2,20)\n",
        "        y = y.type_as(out)\n",
        "        # out = y\n",
        "        #out = torch.cat((out,y.type_as(out)),1)\n",
        "        # print(out)\n",
        "        out = out.unsqueeze(1)\n",
        "        y = y.unsqueeze(1)\n",
        "        #（16,1,25,20）\n",
        "        out = self.conv1(out)\n",
        "        out = self.cbamBlock(out)\n",
        "        y = self.conv3(y)\n",
        "        y = self.cbamBlock(y)\n",
        "        #（16,256,17,12）\n",
        "        out = F.relu(out)\n",
        "        y = F.relu(y)\n",
        "\n",
        "        #out = self.seLayer(out)\n",
        "        # PrimaryCaps\n",
        "        out = self.conv2(out)\n",
        "        y = self.conv4(y)\n",
        "        #(16,256,5,2)\n",
        "        out = F.relu(out)\n",
        "        y = F.relu(y)\n",
        "        out = out.view(batch_size,16,-1)\n",
        "        y = y.view(batch_size,16,-1)\n",
        "        # print(y.shape)\n",
        "        # print(out.shape)\n",
        "        out = torch.cat((out,y),2)\n",
        "        #(16,8,320)\n",
        "        out = squash(out)\n",
        "        # wj(batch_size,8,1152)\n",
        "        out = out.view(out.size(0),out.size(1),-1)\n",
        "        #(16,8,320)\n",
        "        # Capsule\n",
        "        # wj(batch_size,16,10)\n",
        "        out = self.capsule(out)\n",
        "        #(16,16,2)\n",
        "        # wj(batch_size,10,16)\n",
        "        out = out.permute(0,2,1)\n",
        "        # (16,2,16)\n",
        "        # decoder = self.Decoder(out,label)\n",
        "        return out#,decoder\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH1NzWFc2Tr2"
      },
      "source": [
        "# 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FHgwvO-_2PYI"
      },
      "source": [
        "model = CapsuleNet()\n",
        "\"\"\"\n",
        "将前面生成的词向量矩阵拷贝到模型的embedding层\n",
        "这样就自动的可以将输入的word index转为词向量\n",
        "\"\"\"\n",
        "\n",
        "# 训练\n",
        "model.to(DEVICE)\n",
        "\n",
        "# 训练\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "n_epoch = 12\n",
        "\n",
        "best_val_acc = 0\n",
        "#train_iter\n",
        "for epoch in range(n_epoch):\n",
        "    for batch_idx, batch in enumerate(train_iter):\n",
        "        data = batch.Seqs\n",
        "        target = batch.Label\n",
        "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
        "        target = target.to(DEVICE)\n",
        "        data = data.permute(1,0)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # loss = -target*torch.log(out)-(1-target)*torch.log(1-out)\n",
        "        # loss = loss.sum(-1).mean()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        output = torch.sqrt(torch.sum(output*output, 2))\n",
        "        loss1 = target*F.relu(0.9-output)**2 + 0.5*(1-target)*F.relu(output-0.1)**2\n",
        "        loss1 = loss1.sum(dim=1).mean()\n",
        "        #loss2 = ((data-pred_img)**2).mean()\n",
        "        loss = loss1#+0.0005*loss2\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx+1) %100 == 0:\n",
        "            _,y_pre = torch.max(output,-1)\n",
        "            acc = torch.mean((torch.tensor(y_pre == batch.Label,dtype=torch.float)))\n",
        "            print('epoch: %d \\t batch_idx : %d \\t loss: %.4f \\t train acc: %.4f'\n",
        "                  %(epoch,batch_idx,loss,acc))\n",
        "    \n",
        "    val_accs = []\n",
        "    #val_iter\n",
        "    for batch_idx, batch in enumerate(val_iter):\n",
        "        data = batch.Seqs\n",
        "        target = batch.Label\n",
        "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
        "        target = target.to(DEVICE)\n",
        "        data = data.permute(1,0)\n",
        "        out = model(data)\n",
        "        out = torch.sqrt(torch.sum(out*out, 2))\n",
        "        _,y_pre = torch.max(out,-1)\n",
        "        acc = torch.mean((torch.tensor(y_pre == batch.Label,dtype=torch.float)))\n",
        "        val_accs.append(acc)\n",
        "    acc =torch.mean(torch.stack(val_accs))\n",
        "    #acc = np.array(val_accs).mean()\n",
        "    if acc > best_val_acc:\n",
        "        print('val acc : %.4f > %.4f saving model'%(acc,best_val_acc))\n",
        "        torch.save(model.state_dict(), 'params.pkl')\n",
        "        best_val_acc = acc\n",
        "    print('test acc: %.4f'%(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzWIKm3wpWdc"
      },
      "source": [
        "train1,val1,test1 = data.TabularDataset.splits(\n",
        "        path='.', train='train1.csv',validation='test1.csv',test='test1.csv', format='csv',skip_header=True,\n",
        "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
        "TEXT.build_vocab(train,val1,test1)\n",
        "test_iter1 = data.BucketIterator(val1, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
        "                                 shuffle=True,device=DEVICE)\n",
        "\n",
        "train2,val2,test2 = data.TabularDataset.splits(\n",
        "        path='.', train='train2.csv',validation='test2.csv',test='test2.csv', format='csv',skip_header=True,\n",
        "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
        "TEXT.build_vocab(train,val2,test2)\n",
        "test_iter2 = data.BucketIterator(val2, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
        "                                 shuffle=True,device=DEVICE)\n",
        "\n",
        "\n",
        "train3,val3,test3 = data.TabularDataset.splits(\n",
        "        path='.', train='train3.csv',validation='test3.csv',test='test3.csv', format='csv',skip_header=True,\n",
        "        fields=[('Seqs', TEXT), ('Label', LABEL)])\n",
        "TEXT.build_vocab(train,val3,test3)\n",
        "test_iter3 = data.BucketIterator(val3, batch_size=32, sort_key=lambda x: len(x.Seqs), \n",
        "                                 shuffle=True,device=DEVICE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMKE8VD9Nc34"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('params1.pkl'))#, map_location='cpu'))\n",
        "model.eval()\n",
        "model\n",
        "\n",
        "# a = metrics.confusion_matrix(\n",
        "#     all_true,   # array, Gound true (correct) target values\n",
        "#     all_pred,  # array, Estimated targets as returned by a classifier\n",
        "#     labels=None,  # array, List of labels to index the matrix.\n",
        "#     sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n",
        "# )\n",
        "# print(a)\n",
        "# print(metrics.accuracy_score(all_true,all_pred))\n",
        "# print(metrics.roc_auc_score(all_true,all_p))\n",
        "# print(metrics.recall_score(all_true,all_pred))\n",
        "# print(metrics.f1_score(all_true,all_pred))\n",
        "# FN = a[1][0]\n",
        "# FP = a[0][1]\n",
        "# TN = a[0][0]\n",
        "# TP = a[1][1]\n",
        "# print(FN)\n",
        "# print(FP)\n",
        "# print(TN)\n",
        "# print(TP)\n",
        "# print((FN+TP)/(FN+FP+TN+TP))\n",
        "# print(\"se\",TP/ (TP+ FN))\n",
        "# print(\"sp\",TN / (FP + TN))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPqXuJZcNhAG"
      },
      "source": [
        "all_pred1 = []\n",
        "all_true1 = []\n",
        "all_p1 = []\n",
        "for batch_idx, batch in enumerate(test_iter1):\n",
        "        data = batch.Seqs\n",
        "        target = batch.Label\n",
        "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
        "        target = target.to(DEVICE)\n",
        "        data = data.permute(1,0)\n",
        "        out = model(data)\n",
        "        out = torch.sqrt(torch.sum(out*out, 2))\n",
        "        #out1 = F.softmax(out,-1)\n",
        "        # print(out)\n",
        "        #print(out1)\n",
        "        out1 = out[:,1]\n",
        "        # print(out)\n",
        "        _,y_pre = torch.max(out,-1)\n",
        "        \n",
        "        all_p1.extend(list(out1.cpu().detach().numpy()))\n",
        "        all_pred1.extend(list(y_pre.cpu().detach().numpy()))\n",
        "        all_true1.extend(list(batch.Label.cpu().detach().numpy()))\n",
        "print(metrics.accuracy_score(all_true1,all_pred1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l9ml1kYNhtC"
      },
      "source": [
        "all_pred2 = []\n",
        "all_true2 = []\n",
        "all_p2 = []\n",
        "for batch_idx, batch in enumerate(test_iter2):\n",
        "        data = batch.Seqs\n",
        "        target = batch.Label\n",
        "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
        "        target = target.to(DEVICE)\n",
        "        data = data.permute(1,0)\n",
        "        out = model(data)\n",
        "        out = torch.sqrt(torch.sum(out*out, 2))\n",
        "        #out1 = F.softmax(out,-1)\n",
        "        # print(out)\n",
        "        out1 = out[:,1]\n",
        "        #print(out1)\n",
        "        # print(out)\n",
        "        _,y_pre = torch.max(out,-1)\n",
        "        \n",
        "        all_p2.extend(list(out1.cpu().detach().numpy()))\n",
        "        all_pred2.extend(list(y_pre.cpu().detach().numpy()))\n",
        "        all_true2.extend(list(batch.Label.cpu().detach().numpy()))\n",
        "print(metrics.accuracy_score(all_true2,all_pred2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-7C0a_FNlU8"
      },
      "source": [
        "all_pred3 = []\n",
        "all_true3 = []\n",
        "all_p3 = []\n",
        "for batch_idx, batch in enumerate(test_iter3):\n",
        "        data = batch.Seqs\n",
        "        target = batch.Label\n",
        "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
        "        target = target.to(DEVICE)\n",
        "        data = data.permute(1,0)\n",
        "        out = model(data)\n",
        "        out = torch.sqrt(torch.sum(out*out, 2))\n",
        "        #out1 = F.softmax(out,-1)\n",
        "        # print(out)\n",
        "        out1 = out[:,1]\n",
        "        # print(out)\n",
        "        _,y_pre = torch.max(out,-1)\n",
        "        \n",
        "        all_p3.extend(list(out1.cpu().detach().numpy()))\n",
        "        all_pred3.extend(list(y_pre.cpu().detach().numpy()))\n",
        "        all_true3.extend(list(batch.Label.cpu().detach().numpy()))\n",
        "print(metrics.accuracy_score(all_true3,all_pred3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHgo3MprrW32"
      },
      "source": [
        "all_pred4 = []\n",
        "all_true4 = []\n",
        "all_p4 = []\n",
        "for batch_idx, batch in enumerate(test_iter4):\n",
        "        data = batch.Seqs\n",
        "        target = batch.Label\n",
        "        target = torch.sparse.torch.eye(n_class).index_select(dim=0, index=target.cpu().data)\n",
        "        target = target.to(DEVICE)\n",
        "        data = data.permute(1,0)\n",
        "        out = model(data)\n",
        "        out = torch.sqrt(torch.sum(out*out, 2))\n",
        "        #out1 = F.softmax(out,-1)\n",
        "        # print(out)\n",
        "        out1 = out[:,1]\n",
        "        # print(out)\n",
        "        _,y_pre = torch.max(out,-1)\n",
        "        \n",
        "        all_p4.extend(list(out1.cpu().detach().numpy()))\n",
        "        all_pred4.extend(list(y_pre.cpu().detach().numpy()))\n",
        "        all_true4.extend(list(batch.Label.cpu().detach().numpy()))\n",
        "print(metrics.accuracy_score(all_true4,all_pred4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5piTcRiANrup"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "#all_data = [np.random.normal(0, std, 100) for std in range(6, 10)]\n",
        "#\n",
        "# print(all_p1)\n",
        "axes.violinplot(all_p1,all_p2,all_p3,\n",
        "                   showmeans=False,\n",
        "                   showmedians=False\n",
        "                   )\n",
        "axes.set_title('violin plot')\n",
        "\n",
        "# adding horizontal grid lines\n",
        "\n",
        "axes.yaxis.grid(True)\n",
        "#axes.set_xticks([y + 1 for y in range(len(all_data))], )\n",
        "axes.set_xlabel('xlabel')\n",
        "axes.set_ylabel('ylabel')\n",
        "\n",
        "plt.setp(axes, xticks=[y + 1 for y in range(len([all_p1,all_p2,all_p3]))],\n",
        "         xticklabels=['PF1', 'Detected MPs', 'Current MPs'],\n",
        "         )\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}